2025-08-29 20:39:16,061 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-running-shoes-women/p/itm73a37010820b3?pid=SHOH33SDBRFDK5XX&lid=LSTSHOH33SDBRFDK5XXPJLTBA&marketplace=FLIPKART&q=shoes&store=osp&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=127aeba1-ea89-4082-bc13-762f5cb0ebad.SHOH33SDBRFDK5XX.SEARCH&ppt=sp&ppn=sp&ssid=wsgzyrrbo4ucs2kg1756394844172&qH=b0a8b6f820479900
2025-08-29 20:44:15,378 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-running-shoes-women/p/itm73a37010820b3?pid=SHOH33SDBRFDK5XX&lid=LS
2025-08-29 20:44:32,577 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-
2025-08-29 20:44:54,353 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-08-29 20:49:47,385 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-08-29 20:52:14,292 - INFO - Scraping Flipkart | Product: <span class="VU-ZEz">Softride Alexandria Wns Running Shoes For Women<!-- -->  (Black , 8)</span>
2025-08-29 20:52:14,292 - INFO - Price found: <div class="Nx9bqj CxhGGd yKS4la">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-08-29 20:53:21,070 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-08-29 20:53:21,070 - INFO - Price found: <div class="Nx9bqj CxhGGd yKS4la">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-08-29 20:58:44,030 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-08-29 20:58:44,030 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,176</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-09-09 16:44:45,618 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 16:44:45,619 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,131</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-09-09 17:04:18,845 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 17:04:18,845 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,131</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-09-09 17:35:50,074 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 17:35:50,074 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,131</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-09-09 18:00:14,147 - INFO - Scraping Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:00:14,147 - INFO - Price found: <div class="Nx9bqj CxhGGd">₹2,131</div> for https://www.flipkart.com/puma-softride-alexandria-wns-runnin
2025-09-09 18:13:38,765 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:13:39,452 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-09 18:16:13,844 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:16:14,543 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-09 18:39:51,370 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:39:51,989 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-09 18:43:47,330 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:43:48,308 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-09 18:44:10,138 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-09 18:44:11,067 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 16:24:34,452 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 16:24:35,161 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 16:28:36,566 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 16:28:37,213 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 16:28:41,439 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 16:28:42,214 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 16:34:38,080 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 16:34:38,820 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 16:34:45,438 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 16:34:46,293 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 17:08:32,695 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 17:08:33,329 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 17:08:38,904 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 17:08:39,503 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 17:22:46,546 - INFO - Scraped Flipkart | Product: Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-10 17:22:47,157 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 17:22:47,831 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-10 17:22:48,687 - INFO - Scraped Flipkart | Product: SB Heritage Vulc Sneakers For Men  (Blue , 9)
2025-09-10 17:22:49,579 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-10 18:00:20,196 - ERROR - Error scraping https://www.flipkart.com/puma-softride-alexandria-wns-running-shoes-women/p/itm73a37010820b3?pid=SHOH33SDBRFDK5XX&lid=LSTSHOH33SDBRFDK5XXPJLTBA&marketplace=FLIPKART&q=shoes&store=osp&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=127aeba1-ea89-4082-bc13-762f5cb0ebad.SHOH33SDBRFDK5XX.SEARCH&ppt=sp&ppn=sp&ssid=wsgzyrrbo4ucs2kg1756394844172&qH=b0a8b6f820479900: cannot access local variable 'timestamp' where it is not associated with a value
2025-09-10 18:00:20,836 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 18:00:21,566 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-10 18:00:22,494 - INFO - Scraped Flipkart | Product: SB Heritage Vulc Sneakers For Men  (Blue , 9)
2025-09-10 18:00:23,167 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-10 18:01:59,409 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-10 18:02:00,134 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-10 18:02:00,913 - INFO - Scraped Flipkart | Product: SB Heritage Vulc Sneakers For Men  (Blue , 9)
2025-09-10 18:02:01,602 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-12 16:24:57,546 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-12 16:24:58,277 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-12 16:24:59,779 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-12 16:28:50,860 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-12 16:28:51,914 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-12 16:28:53,632 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-12 16:34:31,981 - WARNING - Product is out of stock 
2025-09-12 16:34:32,720 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-12 16:34:33,451 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-12 16:34:34,218 - WARNING - Product is out of stock 
2025-09-12 16:34:34,996 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-12 16:36:17,480 - WARNING - Product is out of stock <span class="VU-ZEz">Softride Alexandria Wns Running Shoes For Women<!-- -->  (Black , 8)</span>
2025-09-12 16:36:18,240 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-12 16:36:19,049 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-12 16:36:19,804 - WARNING - Product is out of stock <span class="VU-ZEz">Heritage Vulc Sneakers For Men<!-- -->  (Blue , 9)</span>
2025-09-12 16:36:20,452 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-12 16:36:50,272 - WARNING - Product is out of stock Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-12 16:36:51,114 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-12 16:36:51,717 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-12 16:36:52,457 - WARNING - Product is out of stock Heritage Vulc Sneakers For Men  (Blue , 9)
2025-09-12 16:36:53,223 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-09-19 12:34:36,085 - WARNING - Product is out of stock Softride Alexandria Wns Running Shoes For Women  (Black , 8)
2025-09-19 12:34:36,893 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-09-19 12:34:37,798 - INFO - Scraped Flipkart | Product: Field General Sneakers For Women  (White , 9)
2025-09-19 12:34:38,576 - WARNING - Product is out of stock Heritage Vulc Sneakers For Men  (Blue , 9)
2025-09-19 12:34:39,431 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:13:13,975 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:13:15,031 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:13:15,825 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:13:16,556 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:13:16,558 - INFO - Closing down clientserver connection
2025-12-19 13:14:51,701 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:14:52,761 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:14:53,671 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:14:54,636 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:14:56,633 - INFO - Closing down clientserver connection
2025-12-19 13:17:24,941 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:17:25,864 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:17:26,739 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:17:27,732 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:17:29,604 - INFO - Closing down clientserver connection
2025-12-19 13:19:24,741 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:19:25,886 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:19:26,895 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:19:27,623 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:19:29,476 - INFO - Closing down clientserver connection
2025-12-19 13:20:59,943 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:21:00,796 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:21:01,659 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:21:02,579 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:21:04,433 - INFO - Closing down clientserver connection
2025-12-19 13:23:11,900 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:23:12,820 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:23:13,813 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:23:14,744 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:23:16,705 - INFO - Closing down clientserver connection
2025-12-19 13:25:15,960 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 13:25:16,748 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 13:25:17,566 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 13:25:18,301 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 13:25:28,493 - INFO - Closing down clientserver connection
2025-12-19 19:39:01,449 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 19:39:02,211 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 19:39:03,219 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 19:39:03,909 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 19:39:14,866 - INFO - Closing down clientserver connection
2025-12-19 20:47:36,817 - INFO - Closing down clientserver connection
2025-12-19 20:48:31,823 - INFO - Closing down clientserver connection
2025-12-19 20:55:12,692 - INFO - Closing down clientserver connection
2025-12-19 20:56:06,171 - INFO - Closing down clientserver connection
2025-12-19 20:57:34,341 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 20:57:35,321 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 20:57:35,957 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 20:57:36,565 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 20:57:42,931 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-19 20:57:47,576 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-19 20:57:47,688 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-19 20:57:52,862 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,264
2025-12-19 20:57:52,975 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-19 20:58:14,119 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x60688ef0f61a <unknown>
#1 0x60688e98c20b <unknown>
#2 0x60688e9dd90f <unknown>
#3 0x60688e9ddb51 <unknown>
#4 0x60688ea2bd84 <unknown>
#5 0x60688ea2920e <unknown>
#6 0x60688e9d0232 <unknown>
#7 0x60688e9d0ee1 <unknown>
#8 0x60688eed83f9 <unknown>
#9 0x60688eedb34d <unknown>
#10 0x60688eec1122 <unknown>
#11 0x60688eedbf2b <unknown>
#12 0x60688eea7c30 <unknown>
#13 0x60688eefcd78 <unknown>
#14 0x60688eefcf4c <unknown>
#15 0x60688ef0e973 <unknown>
#16 0x7d6a7d49caa4 <unknown>
#17 0x7d6a7d529c6c <unknown>

2025-12-19 20:58:14,232 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-19 20:58:35,796 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x5aba4356261a <unknown>
#1 0x5aba42fdf20b <unknown>
#2 0x5aba4303090f <unknown>
#3 0x5aba43030b51 <unknown>
#4 0x5aba4307ed84 <unknown>
#5 0x5aba4307c20e <unknown>
#6 0x5aba43023232 <unknown>
#7 0x5aba43023ee1 <unknown>
#8 0x5aba4352b3f9 <unknown>
#9 0x5aba4352e34d <unknown>
#10 0x5aba43514122 <unknown>
#11 0x5aba4352ef2b <unknown>
#12 0x5aba434fac30 <unknown>
#13 0x5aba4354fd78 <unknown>
#14 0x5aba4354ff4c <unknown>
#15 0x5aba43561973 <unknown>
#16 0x7e903569caa4 <unknown>
#17 0x7e9035729c6c <unknown>

2025-12-19 20:58:39,218 - INFO - Closing down clientserver connection
2025-12-19 21:07:16,280 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 21:07:17,014 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 21:07:17,829 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 21:07:18,397 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 21:07:24,470 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-19 21:07:45,706 - ERROR - Error scraping https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green: Message: 
Stacktrace:
#0 0x625d7fcae61a <unknown>
#1 0x625d7f72b20b <unknown>
#2 0x625d7f77c90f <unknown>
#3 0x625d7f77cb51 <unknown>
#4 0x625d7f7cad84 <unknown>
#5 0x625d7f7c820e <unknown>
#6 0x625d7f76f232 <unknown>
#7 0x625d7f76fee1 <unknown>
#8 0x625d7fc773f9 <unknown>
#9 0x625d7fc7a34d <unknown>
#10 0x625d7fc60122 <unknown>
#11 0x625d7fc7af2b <unknown>
#12 0x625d7fc46c30 <unknown>
#13 0x625d7fc9bd78 <unknown>
#14 0x625d7fc9bf4c <unknown>
#15 0x625d7fcad973 <unknown>
#16 0x7f832149caa4 <unknown>
#17 0x7f8321529c6c <unknown>

2025-12-19 21:07:45,787 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-19 21:07:50,086 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,264
2025-12-19 21:07:50,214 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-19 21:07:54,791 - INFO - Scraped Ajio Product: Field General Running Shoes | Price: ₹6,173
2025-12-19 21:07:54,901 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-19 21:07:59,160 - INFO - Scraped Ajio Product: Men Killshot 2 Leather Lace-Up Tennis Shoes | Price: ₹6,076
2025-12-19 21:08:19,841 - ERROR - [CAST_INVALID_INPUT] The value 'MRP' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o183.parquet.
: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'MRP' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"cast" was called from
/home/moksh/Desktop/smart-price-tracker/src/process_data.py:27

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
	at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
	at org.apache.spark.scheduler.Task.run(Task.scala:147)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
		at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
		at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
		at org.apache.spark.scheduler.Task.run(Task.scala:147)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		... 1 more

2025-12-19 21:08:19,854 - INFO - Closing down clientserver connection
2025-12-19 21:22:34,134 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 21:22:35,105 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 21:22:35,833 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 21:22:36,594 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 21:22:45,616 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-19 21:22:52,237 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-19 21:22:52,429 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-19 21:22:59,074 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,264
2025-12-19 21:22:59,301 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-19 21:23:06,354 - INFO - Scraped Ajio Product: Field General Running Shoes | Price: ₹6,173
2025-12-19 21:23:06,613 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-19 21:23:28,055 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x5c636c34961a <unknown>
#1 0x5c636bdc620b <unknown>
#2 0x5c636be1790f <unknown>
#3 0x5c636be17b51 <unknown>
#4 0x5c636be65d84 <unknown>
#5 0x5c636be6320e <unknown>
#6 0x5c636be0a232 <unknown>
#7 0x5c636be0aee1 <unknown>
#8 0x5c636c3123f9 <unknown>
#9 0x5c636c31534d <unknown>
#10 0x5c636c2fb122 <unknown>
#11 0x5c636c315f2b <unknown>
#12 0x5c636c2e1c30 <unknown>
#13 0x5c636c336d78 <unknown>
#14 0x5c636c336f4c <unknown>
#15 0x5c636c348973 <unknown>
#16 0x7f85c5a9caa4 <unknown>
#17 0x7f85c5b29c6c <unknown>

2025-12-19 21:23:53,339 - ERROR - [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o182.parquet.
: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"cast" was called from
/home/moksh/Desktop/smart-price-tracker/src/process_data.py:27

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
	at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
	at org.apache.spark.scheduler.Task.run(Task.scala:147)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
		at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
		at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
		at org.apache.spark.scheduler.Task.run(Task.scala:147)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		... 1 more

2025-12-19 21:23:53,747 - INFO - Closing down clientserver connection
2025-12-19 21:30:56,381 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-19 21:30:57,260 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-19 21:30:58,033 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-19 21:30:58,724 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-19 21:31:07,432 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-19 21:31:14,345 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-19 21:31:14,534 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-19 21:31:20,527 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,264
2025-12-19 21:31:20,753 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-19 21:31:27,748 - INFO - Scraped Ajio Product: Field General Running Shoes | Price: ₹6,173
2025-12-19 21:31:27,970 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-19 21:31:49,392 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x63d62b58661a <unknown>
#1 0x63d62b00320b <unknown>
#2 0x63d62b05490f <unknown>
#3 0x63d62b054b51 <unknown>
#4 0x63d62b0a2d84 <unknown>
#5 0x63d62b0a020e <unknown>
#6 0x63d62b047232 <unknown>
#7 0x63d62b047ee1 <unknown>
#8 0x63d62b54f3f9 <unknown>
#9 0x63d62b55234d <unknown>
#10 0x63d62b538122 <unknown>
#11 0x63d62b552f2b <unknown>
#12 0x63d62b51ec30 <unknown>
#13 0x63d62b573d78 <unknown>
#14 0x63d62b573f4c <unknown>
#15 0x63d62b585973 <unknown>
#16 0x7f9fbbe9caa4 <unknown>
#17 0x7f9fbbf29c6c <unknown>

2025-12-19 21:32:32,604 - INFO - Closing down clientserver connection
2025-12-21 18:15:10,984 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 18:15:11,770 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 18:15:12,659 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 18:15:13,281 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 18:15:20,716 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 18:20:12,504 - ERROR - Error scraping https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green: Message: 
Stacktrace:
#0 0x5b3f6a30661a <unknown>
#1 0x5b3f69d8320b <unknown>
#2 0x5b3f69dd490f <unknown>
#3 0x5b3f69dd4b51 <unknown>
#4 0x5b3f69e22d84 <unknown>
#5 0x5b3f69e2020e <unknown>
#6 0x5b3f69dc7232 <unknown>
#7 0x5b3f69dc7ee1 <unknown>
#8 0x5b3f6a2cf3f9 <unknown>
#9 0x5b3f6a2d234d <unknown>
#10 0x5b3f6a2b8122 <unknown>
#11 0x5b3f6a2d2f2b <unknown>
#12 0x5b3f6a29ec30 <unknown>
#13 0x5b3f6a2f3d78 <unknown>
#14 0x5b3f6a2f3f4c <unknown>
#15 0x5b3f6a305973 <unknown>
#16 0x78756769caa4 <unknown>
#17 0x787567729c6c <unknown>

2025-12-21 18:20:12,618 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 18:20:33,694 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x5ba85dcc661a <unknown>
#1 0x5ba85d74320b <unknown>
#2 0x5ba85d79490f <unknown>
#3 0x5ba85d794b51 <unknown>
#4 0x5ba85d7e2d84 <unknown>
#5 0x5ba85d7e020e <unknown>
#6 0x5ba85d787232 <unknown>
#7 0x5ba85d787ee1 <unknown>
#8 0x5ba85dc8f3f9 <unknown>
#9 0x5ba85dc9234d <unknown>
#10 0x5ba85dc78122 <unknown>
#11 0x5ba85dc92f2b <unknown>
#12 0x5ba85dc5ec30 <unknown>
#13 0x5ba85dcb3d78 <unknown>
#14 0x5ba85dcb3f4c <unknown>
#15 0x5ba85dcc5973 <unknown>
#16 0x74f5a429caa4 <unknown>
#17 0x74f5a4329c6c <unknown>

2025-12-21 18:20:33,791 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 18:23:10,214 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x61d3e6cda61a <unknown>
#1 0x61d3e675720b <unknown>
#2 0x61d3e67a890f <unknown>
#3 0x61d3e67a8b51 <unknown>
#4 0x61d3e67f6d84 <unknown>
#5 0x61d3e67f420e <unknown>
#6 0x61d3e679b232 <unknown>
#7 0x61d3e679bee1 <unknown>
#8 0x61d3e6ca33f9 <unknown>
#9 0x61d3e6ca634d <unknown>
#10 0x61d3e6c8c122 <unknown>
#11 0x61d3e6ca6f2b <unknown>
#12 0x61d3e6c72c30 <unknown>
#13 0x61d3e6cc7d78 <unknown>
#14 0x61d3e6cc7f4c <unknown>
#15 0x61d3e6cd9973 <unknown>
#16 0x78827409caa4 <unknown>
#17 0x788274129c6c <unknown>

2025-12-21 18:23:10,294 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 18:25:45,452 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x64d9b1fc461a <unknown>
#1 0x64d9b1a4120b <unknown>
#2 0x64d9b1a9290f <unknown>
#3 0x64d9b1a92b51 <unknown>
#4 0x64d9b1ae0d84 <unknown>
#5 0x64d9b1ade20e <unknown>
#6 0x64d9b1a85232 <unknown>
#7 0x64d9b1a85ee1 <unknown>
#8 0x64d9b1f8d3f9 <unknown>
#9 0x64d9b1f9034d <unknown>
#10 0x64d9b1f76122 <unknown>
#11 0x64d9b1f90f2b <unknown>
#12 0x64d9b1f5cc30 <unknown>
#13 0x64d9b1fb1d78 <unknown>
#14 0x64d9b1fb1f4c <unknown>
#15 0x64d9b1fc3973 <unknown>
#16 0x710c1629caa4 <unknown>
#17 0x710c16329c6c <unknown>

2025-12-21 18:26:34,460 - INFO - Closing down clientserver connection
2025-12-21 19:13:49,774 - INFO - Closing down clientserver connection
2025-12-21 19:14:51,870 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 19:14:53,229 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 19:14:54,018 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 19:14:55,596 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 19:15:06,145 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 19:15:12,677 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-21 19:15:12,898 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 19:15:20,639 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,264
2025-12-21 19:15:20,863 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 19:15:28,035 - INFO - Scraped Ajio Product: Field General Running Shoes | Price: ₹6,173
2025-12-21 19:15:28,291 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 19:15:34,798 - INFO - Scraped Ajio Product: Men Killshot 2 Leather Lace-Up Tennis Shoes | Price: ₹6,076
2025-12-21 19:16:20,191 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o191.agg.
: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703;
'Aggregate ['max('timestamp) AS max_ts#121]
+- Relation [product_name#116,price_date#117,selling_price#118,avg_price_7d#119,signal#120] parquet

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
		at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
		at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 22 more

2025-12-21 19:16:21,273 - INFO - Closing down clientserver connection
2025-12-21 19:24:11,973 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 19:24:13,607 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 19:24:14,265 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 19:24:15,154 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 19:24:24,352 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 19:24:32,301 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-21 19:24:32,525 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 19:27:08,535 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x5baf5d09961a <unknown>
#1 0x5baf5cb1620b <unknown>
#2 0x5baf5cb6790f <unknown>
#3 0x5baf5cb67b51 <unknown>
#4 0x5baf5cbb5d84 <unknown>
#5 0x5baf5cbb320e <unknown>
#6 0x5baf5cb5a232 <unknown>
#7 0x5baf5cb5aee1 <unknown>
#8 0x5baf5d0623f9 <unknown>
#9 0x5baf5d06534d <unknown>
#10 0x5baf5d04b122 <unknown>
#11 0x5baf5d065f2b <unknown>
#12 0x5baf5d031c30 <unknown>
#13 0x5baf5d086d78 <unknown>
#14 0x5baf5d086f4c <unknown>
#15 0x5baf5d098973 <unknown>
#16 0x7adc51c9caa4 <unknown>
#17 0x7adc51d29c6c <unknown>

2025-12-21 19:27:08,695 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 19:29:44,213 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x62d46642e61a <unknown>
#1 0x62d465eab20b <unknown>
#2 0x62d465efc90f <unknown>
#3 0x62d465efcb51 <unknown>
#4 0x62d465f4ad84 <unknown>
#5 0x62d465f4820e <unknown>
#6 0x62d465eef232 <unknown>
#7 0x62d465eefee1 <unknown>
#8 0x62d4663f73f9 <unknown>
#9 0x62d4663fa34d <unknown>
#10 0x62d4663e0122 <unknown>
#11 0x62d4663faf2b <unknown>
#12 0x62d4663c6c30 <unknown>
#13 0x62d46641bd78 <unknown>
#14 0x62d46641bf4c <unknown>
#15 0x62d46642d973 <unknown>
#16 0x77598029caa4 <unknown>
#17 0x775980329c6c <unknown>

2025-12-21 19:29:44,358 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 19:30:06,392 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x5e557bb5b61a <unknown>
#1 0x5e557b5d820b <unknown>
#2 0x5e557b62990f <unknown>
#3 0x5e557b629b51 <unknown>
#4 0x5e557b677d84 <unknown>
#5 0x5e557b67520e <unknown>
#6 0x5e557b61c232 <unknown>
#7 0x5e557b61cee1 <unknown>
#8 0x5e557bb243f9 <unknown>
#9 0x5e557bb2734d <unknown>
#10 0x5e557bb0d122 <unknown>
#11 0x5e557bb27f2b <unknown>
#12 0x5e557baf3c30 <unknown>
#13 0x5e557bb48d78 <unknown>
#14 0x5e557bb48f4c <unknown>
#15 0x5e557bb5a973 <unknown>
#16 0x7f9295c9caa4 <unknown>
#17 0x7f9295d29c6c <unknown>

2025-12-21 19:30:51,955 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o191.agg.
: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703;
'Aggregate ['max('timestamp) AS max_ts#121]
+- Relation [product_name#116,price_date#117,selling_price#118,avg_price_7d#119,signal#120] parquet

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
		at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
		at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 22 more

2025-12-21 19:30:53,116 - INFO - Closing down clientserver connection
2025-12-21 19:33:00,708 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 19:33:01,530 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 19:33:03,104 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 19:35:18,803 - ERROR - Error scraping https://www.flipkart.com/nike-killshot-2-leather-sneakers-men/p/itm28a181bebad18?pid=SHOHFF25UUMSAKZY&lid=LSTSHOHFF25UUMSAKZYGFDTPW&marketplace=FLIPKART&q=killshot+2&store=osp%2Fcil%2Fe1f&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=565272a9-a421-4a4d-a80b-bb66db86d404.SHOHFF25UUMSAKZY.SEARCH&ppt=sp&ppn=sp&ssid=dscy7gk029gx5iww1757505100423&qH=5a1643cee092c11e: HTTPSConnectionPool(host='www.flipkart.com', port=443): Max retries exceeded with url: /nike-killshot-2-leather-sneakers-men/p/itm28a181bebad18?pid=SHOHFF25UUMSAKZY&lid=LSTSHOHFF25UUMSAKZYGFDTPW&marketplace=FLIPKART&q=killshot+2&store=osp%2Fcil%2Fe1f&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=565272a9-a421-4a4d-a80b-bb66db86d404.SHOHFF25UUMSAKZY.SEARCH&ppt=sp&ppn=sp&ssid=dscy7gk029gx5iww1757505100423&qH=5a1643cee092c11e (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x703a3ef938b0>, 'Connection to www.flipkart.com timed out. (connect timeout=None)'))
2025-12-21 19:35:27,630 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 19:35:50,096 - ERROR - Error scraping https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green: Message: 
Stacktrace:
#0 0x58c05228f61a <unknown>
#1 0x58c051d0c20b <unknown>
#2 0x58c051d5d90f <unknown>
#3 0x58c051d5db51 <unknown>
#4 0x58c051dabd84 <unknown>
#5 0x58c051da920e <unknown>
#6 0x58c051d50232 <unknown>
#7 0x58c051d50ee1 <unknown>
#8 0x58c0522583f9 <unknown>
#9 0x58c05225b34d <unknown>
#10 0x58c052241122 <unknown>
#11 0x58c05225bf2b <unknown>
#12 0x58c052227c30 <unknown>
#13 0x58c05227cd78 <unknown>
#14 0x58c05227cf4c <unknown>
#15 0x58c05228e973 <unknown>
#16 0x79cef389caa4 <unknown>
#17 0x79cef3929c6c <unknown>

2025-12-21 19:35:50,282 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 19:36:12,306 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x63e61ef1a61a <unknown>
#1 0x63e61e99720b <unknown>
#2 0x63e61e9e890f <unknown>
#3 0x63e61e9e8b51 <unknown>
#4 0x63e61ea36d84 <unknown>
#5 0x63e61ea3420e <unknown>
#6 0x63e61e9db232 <unknown>
#7 0x63e61e9dbee1 <unknown>
#8 0x63e61eee33f9 <unknown>
#9 0x63e61eee634d <unknown>
#10 0x63e61eecc122 <unknown>
#11 0x63e61eee6f2b <unknown>
#12 0x63e61eeb2c30 <unknown>
#13 0x63e61ef07d78 <unknown>
#14 0x63e61ef07f4c <unknown>
#15 0x63e61ef19973 <unknown>
#16 0x704e3ba9caa4 <unknown>
#17 0x704e3bb29c6c <unknown>

2025-12-21 19:36:13,504 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 19:36:35,527 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x58411bb1061a <unknown>
#1 0x58411b58d20b <unknown>
#2 0x58411b5de90f <unknown>
#3 0x58411b5deb51 <unknown>
#4 0x58411b62cd84 <unknown>
#5 0x58411b62a20e <unknown>
#6 0x58411b5d1232 <unknown>
#7 0x58411b5d1ee1 <unknown>
#8 0x58411bad93f9 <unknown>
#9 0x58411badc34d <unknown>
#10 0x58411bac2122 <unknown>
#11 0x58411badcf2b <unknown>
#12 0x58411baa8c30 <unknown>
#13 0x58411bafdd78 <unknown>
#14 0x58411bafdf4c <unknown>
#15 0x58411bb0f973 <unknown>
#16 0x71ff64a9caa4 <unknown>
#17 0x71ff64b29c6c <unknown>

2025-12-21 19:36:35,695 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 19:36:57,968 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x5f92e6b7f61a <unknown>
#1 0x5f92e65fc20b <unknown>
#2 0x5f92e664d90f <unknown>
#3 0x5f92e664db51 <unknown>
#4 0x5f92e669bd84 <unknown>
#5 0x5f92e669920e <unknown>
#6 0x5f92e6640232 <unknown>
#7 0x5f92e6640ee1 <unknown>
#8 0x5f92e6b483f9 <unknown>
#9 0x5f92e6b4b34d <unknown>
#10 0x5f92e6b31122 <unknown>
#11 0x5f92e6b4bf2b <unknown>
#12 0x5f92e6b17c30 <unknown>
#13 0x5f92e6b6cd78 <unknown>
#14 0x5f92e6b6cf4c <unknown>
#15 0x5f92e6b7e973 <unknown>
#16 0x7ed44489caa4 <unknown>
#17 0x7ed444929c6c <unknown>

2025-12-21 19:37:46,979 - ERROR - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o191.agg.
: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`signal`, `price_date`, `product_name`, `avg_price_7d`, `selling_price`]. SQLSTATE: 42703;
'Aggregate ['max('timestamp) AS max_ts#121]
+- Relation [product_name#116,price_date#117,selling_price#118,avg_price_7d#119,signal#120] parquet

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
	at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)
	at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)
	at org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
		at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
		at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)
		at scala.collection.immutable.Vector.foreach(Vector.scala:2125)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 22 more

2025-12-21 19:37:51,834 - ERROR - [TRAILING_COMMA_IN_SELECT] Trailing comma detected in SELECT clause. Remove the trailing comma before the FROM clause. SQLSTATE: 42601
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o40.sql.
: org.apache.spark.sql.AnalysisException: [TRAILING_COMMA_IN_SELECT] Trailing comma detected in SELECT clause. Remove the trailing comma before the FROM clause. SQLSTATE: 42601; line 7 pos 8
	at org.apache.spark.sql.errors.QueryCompilationErrors$.trailingCommaInSelectError(QueryCompilationErrors.scala:372)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkTrailingCommaInSelect(CheckAnalysis.scala:224)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkTrailingCommaInSelect$(CheckAnalysis.scala:208)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkTrailingCommaInSelect(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1454)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1697)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(cteOperators.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.trailingCommaInSelectError(QueryCompilationErrors.scala:372)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkTrailingCommaInSelect(CheckAnalysis.scala:224)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkTrailingCommaInSelect$(CheckAnalysis.scala:208)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkTrailingCommaInSelect(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1454)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1697)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(cteOperators.scala:90)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more

2025-12-21 19:37:51,939 - INFO - Closing down clientserver connection
2025-12-21 19:39:36,383 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 19:39:37,179 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 19:39:38,872 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 19:39:39,611 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 19:39:48,509 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 19:40:10,447 - ERROR - Error scraping https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green: Message: 
Stacktrace:
#0 0x5ce49345561a <unknown>
#1 0x5ce492ed220b <unknown>
#2 0x5ce492f2390f <unknown>
#3 0x5ce492f23b51 <unknown>
#4 0x5ce492f71d84 <unknown>
#5 0x5ce492f6f20e <unknown>
#6 0x5ce492f16232 <unknown>
#7 0x5ce492f16ee1 <unknown>
#8 0x5ce49341e3f9 <unknown>
#9 0x5ce49342134d <unknown>
#10 0x5ce493407122 <unknown>
#11 0x5ce493421f2b <unknown>
#12 0x5ce4933edc30 <unknown>
#13 0x5ce493442d78 <unknown>
#14 0x5ce493442f4c <unknown>
#15 0x5ce493454973 <unknown>
#16 0x713e23a9caa4 <unknown>
#17 0x713e23b29c6c <unknown>

2025-12-21 19:40:10,619 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 19:40:32,811 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x57344673d61a <unknown>
#1 0x5734461ba20b <unknown>
#2 0x57344620b90f <unknown>
#3 0x57344620bb51 <unknown>
#4 0x573446259d84 <unknown>
#5 0x57344625720e <unknown>
#6 0x5734461fe232 <unknown>
#7 0x5734461feee1 <unknown>
#8 0x5734467063f9 <unknown>
#9 0x57344670934d <unknown>
#10 0x5734466ef122 <unknown>
#11 0x573446709f2b <unknown>
#12 0x5734466d5c30 <unknown>
#13 0x57344672ad78 <unknown>
#14 0x57344672af4c <unknown>
#15 0x57344673c973 <unknown>
#16 0x7d14d009caa4 <unknown>
#17 0x7d14d0129c6c <unknown>

2025-12-21 19:40:33,006 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 19:40:55,522 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x5956a8d7661a <unknown>
#1 0x5956a87f320b <unknown>
#2 0x5956a884490f <unknown>
#3 0x5956a8844b51 <unknown>
#4 0x5956a8892d84 <unknown>
#5 0x5956a889020e <unknown>
#6 0x5956a8837232 <unknown>
#7 0x5956a8837ee1 <unknown>
#8 0x5956a8d3f3f9 <unknown>
#9 0x5956a8d4234d <unknown>
#10 0x5956a8d28122 <unknown>
#11 0x5956a8d42f2b <unknown>
#12 0x5956a8d0ec30 <unknown>
#13 0x5956a8d63d78 <unknown>
#14 0x5956a8d63f4c <unknown>
#15 0x5956a8d75973 <unknown>
#16 0x740b1ba9caa4 <unknown>
#17 0x740b1bb29c6c <unknown>

2025-12-21 19:40:55,686 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 19:41:18,469 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x5e9f3dcac61a <unknown>
#1 0x5e9f3d72920b <unknown>
#2 0x5e9f3d77a90f <unknown>
#3 0x5e9f3d77ab51 <unknown>
#4 0x5e9f3d7c8d84 <unknown>
#5 0x5e9f3d7c620e <unknown>
#6 0x5e9f3d76d232 <unknown>
#7 0x5e9f3d76dee1 <unknown>
#8 0x5e9f3dc753f9 <unknown>
#9 0x5e9f3dc7834d <unknown>
#10 0x5e9f3dc5e122 <unknown>
#11 0x5e9f3dc78f2b <unknown>
#12 0x5e9f3dc44c30 <unknown>
#13 0x5e9f3dc99d78 <unknown>
#14 0x5e9f3dc99f4c <unknown>
#15 0x5e9f3dcab973 <unknown>
#16 0x7f17b289caa4 <unknown>
#17 0x7f17b2929c6c <unknown>

2025-12-21 19:42:22,106 - ERROR - [AMBIGUOUS_REFERENCE] Reference `price_date` is ambiguous, could be: [`cte`.`price_date`, `cte`.`price_date`]. SQLSTATE: 42704
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o40.sql.
: org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `price_date` is ambiguous, could be: [`cte`.`price_date`, `cte`.`price_date`]. SQLSTATE: 42704; line 14 pos 25
	at org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)
	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1259)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1256)
	at org.apache.spark.sql.catalyst.expressions.WindowExpression.mapChildren(windowExpressions.scala:345)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1697)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(cteOperators.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)
		at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)
		at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1259)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1256)
		at org.apache.spark.sql.catalyst.expressions.WindowExpression.mapChildren(windowExpressions.scala:345)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1697)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.plans.logical.CTERelationDef.mapChildren(cteOperators.scala:90)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at scala.collection.immutable.Vector1.map(Vector.scala:2141)
		at scala.collection.immutable.Vector1.map(Vector.scala:386)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more

2025-12-21 19:42:22,212 - INFO - Closing down clientserver connection
2025-12-21 19:45:30,591 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 19:45:31,708 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 19:45:32,845 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 19:45:33,848 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 19:45:42,065 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 19:46:04,451 - ERROR - Error scraping https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green: Message: 
Stacktrace:
#0 0x6474eb7cc61a <unknown>
#1 0x6474eb24920b <unknown>
#2 0x6474eb29a90f <unknown>
#3 0x6474eb29ab51 <unknown>
#4 0x6474eb2e8d84 <unknown>
#5 0x6474eb2e620e <unknown>
#6 0x6474eb28d232 <unknown>
#7 0x6474eb28dee1 <unknown>
#8 0x6474eb7953f9 <unknown>
#9 0x6474eb79834d <unknown>
#10 0x6474eb77e122 <unknown>
#11 0x6474eb798f2b <unknown>
#12 0x6474eb764c30 <unknown>
#13 0x6474eb7b9d78 <unknown>
#14 0x6474eb7b9f4c <unknown>
#15 0x6474eb7cb973 <unknown>
#16 0x7f7deb89caa4 <unknown>
#17 0x7f7deb929c6c <unknown>

2025-12-21 19:46:04,598 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 19:46:26,588 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x5cad6f8f461a <unknown>
#1 0x5cad6f37120b <unknown>
#2 0x5cad6f3c290f <unknown>
#3 0x5cad6f3c2b51 <unknown>
#4 0x5cad6f410d84 <unknown>
#5 0x5cad6f40e20e <unknown>
#6 0x5cad6f3b5232 <unknown>
#7 0x5cad6f3b5ee1 <unknown>
#8 0x5cad6f8bd3f9 <unknown>
#9 0x5cad6f8c034d <unknown>
#10 0x5cad6f8a6122 <unknown>
#11 0x5cad6f8c0f2b <unknown>
#12 0x5cad6f88cc30 <unknown>
#13 0x5cad6f8e1d78 <unknown>
#14 0x5cad6f8e1f4c <unknown>
#15 0x5cad6f8f3973 <unknown>
#16 0x74b57cc9caa4 <unknown>
#17 0x74b57cd29c6c <unknown>

2025-12-21 19:46:26,733 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 19:46:48,674 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x5c07a911f61a <unknown>
#1 0x5c07a8b9c20b <unknown>
#2 0x5c07a8bed90f <unknown>
#3 0x5c07a8bedb51 <unknown>
#4 0x5c07a8c3bd84 <unknown>
#5 0x5c07a8c3920e <unknown>
#6 0x5c07a8be0232 <unknown>
#7 0x5c07a8be0ee1 <unknown>
#8 0x5c07a90e83f9 <unknown>
#9 0x5c07a90eb34d <unknown>
#10 0x5c07a90d1122 <unknown>
#11 0x5c07a90ebf2b <unknown>
#12 0x5c07a90b7c30 <unknown>
#13 0x5c07a910cd78 <unknown>
#14 0x5c07a910cf4c <unknown>
#15 0x5c07a911e973 <unknown>
#16 0x767d0909caa4 <unknown>
#17 0x767d09129c6c <unknown>

2025-12-21 19:46:48,820 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 19:49:23,694 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x64960aa1e61a <unknown>
#1 0x64960a49b20b <unknown>
#2 0x64960a4ec90f <unknown>
#3 0x64960a4ecb51 <unknown>
#4 0x64960a53ad84 <unknown>
#5 0x64960a53820e <unknown>
#6 0x64960a4df232 <unknown>
#7 0x64960a4dfee1 <unknown>
#8 0x64960a9e73f9 <unknown>
#9 0x64960a9ea34d <unknown>
#10 0x64960a9d0122 <unknown>
#11 0x64960a9eaf2b <unknown>
#12 0x64960a9b6c30 <unknown>
#13 0x64960aa0bd78 <unknown>
#14 0x64960aa0bf4c <unknown>
#15 0x64960aa1d973 <unknown>
#16 0x7bce07a9caa4 <unknown>
#17 0x7bce07b29c6c <unknown>

2025-12-21 19:50:33,782 - ERROR - [AMBIGUOUS_REFERENCE] Reference `avg_price_7d` is ambiguous, could be: [`rolling_avg`.`avg_price_7d`, `rolling_avg`.`avg_price_7d`]. SQLSTATE: 42704
Traceback (most recent call last):
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
  File "/home/moksh/.conda/envs/smart-price-tracker/lib/python3.10/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o40.sql.
: org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `avg_price_7d` is ambiguous, could be: [`rolling_avg`.`avg_price_7d`, `rolling_avg`.`avg_price_7d`]. SQLSTATE: 42704; line 18 pos 14
	at org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)
	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)
	at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)
		at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)
		at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)
		at org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:136)
		at scala.collection.immutable.Vector1.map(Vector.scala:2141)
		at scala.collection.immutable.Vector1.map(Vector.scala:386)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:136)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more

2025-12-21 19:50:33,896 - INFO - Closing down clientserver connection
2025-12-21 20:02:11,004 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-21 20:02:12,312 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-21 20:02:13,412 - WARNING - Product is out of stock Sneakers For Women  (Pink , 7)
2025-12-21 20:02:14,359 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-21 20:02:25,027 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-21 20:02:31,506 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,608
2025-12-21 20:02:31,729 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-21 20:02:53,715 - ERROR - Error scraping https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?: Message: 
Stacktrace:
#0 0x57885215361a <unknown>
#1 0x578851bd020b <unknown>
#2 0x578851c2190f <unknown>
#3 0x578851c21b51 <unknown>
#4 0x578851c6fd84 <unknown>
#5 0x578851c6d20e <unknown>
#6 0x578851c14232 <unknown>
#7 0x578851c14ee1 <unknown>
#8 0x57885211c3f9 <unknown>
#9 0x57885211f34d <unknown>
#10 0x578852105122 <unknown>
#11 0x57885211ff2b <unknown>
#12 0x5788520ebc30 <unknown>
#13 0x578852140d78 <unknown>
#14 0x578852140f4c <unknown>
#15 0x578852152973 <unknown>
#16 0x71809209caa4 <unknown>
#17 0x718092129c6c <unknown>

2025-12-21 20:02:54,874 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-21 20:05:29,968 - ERROR - Error scraping https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?: Message: 
Stacktrace:
#0 0x5e237fbea61a <unknown>
#1 0x5e237f66720b <unknown>
#2 0x5e237f6b890f <unknown>
#3 0x5e237f6b8b51 <unknown>
#4 0x5e237f706d84 <unknown>
#5 0x5e237f70420e <unknown>
#6 0x5e237f6ab232 <unknown>
#7 0x5e237f6abee1 <unknown>
#8 0x5e237fbb33f9 <unknown>
#9 0x5e237fbb634d <unknown>
#10 0x5e237fb9c122 <unknown>
#11 0x5e237fbb6f2b <unknown>
#12 0x5e237fb82c30 <unknown>
#13 0x5e237fbd7d78 <unknown>
#14 0x5e237fbd7f4c <unknown>
#15 0x5e237fbe9973 <unknown>
#16 0x7fa94129caa4 <unknown>
#17 0x7fa941329c6c <unknown>

2025-12-21 20:05:30,081 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-21 20:05:51,284 - ERROR - Error scraping https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?: Message: 
Stacktrace:
#0 0x58e1e959461a <unknown>
#1 0x58e1e901120b <unknown>
#2 0x58e1e906290f <unknown>
#3 0x58e1e9062b51 <unknown>
#4 0x58e1e90b0d84 <unknown>
#5 0x58e1e90ae20e <unknown>
#6 0x58e1e9055232 <unknown>
#7 0x58e1e9055ee1 <unknown>
#8 0x58e1e955d3f9 <unknown>
#9 0x58e1e956034d <unknown>
#10 0x58e1e9546122 <unknown>
#11 0x58e1e9560f2b <unknown>
#12 0x58e1e952cc30 <unknown>
#13 0x58e1e9581d78 <unknown>
#14 0x58e1e9581f4c <unknown>
#15 0x58e1e9593973 <unknown>
#16 0x7e7a91e9caa4 <unknown>
#17 0x7e7a91f29c6c <unknown>

2025-12-21 20:06:44,846 - INFO - Closing down clientserver connection
2025-12-22 21:16:32,240 - WARNING - Product is out of stock Downshifter 13 Running Shoes For Women  (Black , 6)
2025-12-22 21:16:33,152 - INFO - Scraped Flipkart | Product: C1TY Sneakers For Men  (Grey , 7)
2025-12-22 21:16:33,850 - INFO - Scraped Flipkart | Product: Sneakers For Women  (Pink , 7)
2025-12-22 21:16:34,424 - INFO - Scraped Flipkart | Product: Killshot 2 Leather Sneakers For Men  (White , 8.5)
2025-12-22 21:16:41,726 - INFO - Fetching product from https://www.ajio.com/nike-men-c1ty-low-top-lace-up-basket-ball-shoes/p/469695776_green
2025-12-22 21:16:49,192 - INFO - Scraped Ajio Product: Men C1TY Low-Top Lace-Up Basket Ball Shoes | Price: ₹6,782
2025-12-22 21:16:49,374 - INFO - Fetching product from https://www.ajio.com/nike-downshifter-13-running-shoes/p/469581864_black?
2025-12-22 21:16:54,114 - INFO - Scraped Ajio Product: Downshifter 13 Running Shoes | Price: ₹3,350
2025-12-22 21:16:54,292 - INFO - Fetching product from https://www.ajio.com/nike-field-general-running-shoes/p/469763433_blackgrey?
2025-12-22 21:16:59,013 - INFO - Scraped Ajio Product: Field General Running Shoes | Price: ₹6,521
2025-12-22 21:16:59,195 - INFO - Fetching product from https://www.ajio.com/nike-men-killshot-2-leather-lace-up-tennis-shoes/p/469759270_white?
2025-12-22 21:17:04,170 - INFO - Scraped Ajio Product: Men Killshot 2 Leather Lace-Up Tennis Shoes | Price: ₹6,076
2025-12-22 21:18:02,833 - INFO - Closing down clientserver connection
